{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralMachineTranslation_En-Pt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW7iJsQNEbU8",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNX14AtvEprj",
        "colab_type": "code",
        "outputId": "76a68592-be37-4bc9-a3e3-63f970a05672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrow0G4TEr2W",
        "colab_type": "text"
      },
      "source": [
        "# Load data\n",
        "http://www.manythings.org/anki/por-eng.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a8d8P4NE8B1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "###From Upload System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0ie07vjD35X",
        "colab_type": "code",
        "outputId": "343fc1cb-7862-4bb7-cc21-1532540d325e",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7d21878e-6444-4179-8b00-9601fb6bf605\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7d21878e-6444-4179-8b00-9601fb6bf605\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWByOtjjAWl9",
        "colab_type": "text"
      },
      "source": [
        "###From repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLPW5kahAd_L",
        "colab_type": "code",
        "outputId": "367314ce-7691-49ac-b1f5-a020dc7fd7a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "!git clone https://github.com/ufrpe-mineracao-textos/projeto-de-mineracao-20192-traducao-de-texto.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'projeto-de-mineracao-20192-traducao-de-texto'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 1194 (delta 33), reused 91 (delta 11), pack-reused 1060\u001b[K\n",
            "Receiving objects: 100% (1194/1194), 137.71 MiB | 44.41 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VRaB4uFFSR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#file = open('por.txt', mode='rt', encoding='utf-8')\n",
        "file = open('/content/projeto-de-mineracao-20192-traducao-de-texto/datasets/por.txt', mode='rt', encoding='utf-8')\n",
        "data = file.read()\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsvLUZUC_TaO",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QZIDZaC_A4n",
        "colab_type": "text"
      },
      "source": [
        "###Split lines and phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxxGatPo6hJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = data.strip().split('\\n')\n",
        "pairs = [line.split('\\t') for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE5oTqs1_4Uo",
        "colab_type": "text"
      },
      "source": [
        "###Clean lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edbprNCOJfbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned = list()\n",
        "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "for pair in pairs:\n",
        "  clean_pair = list()\n",
        "  for line in pair:\n",
        "    # normalize unicode characters\n",
        "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "    line = line.decode('UTF-8')\n",
        "    # tokenize on white space\n",
        "    line = line.split()\n",
        "    # convert to lowercase\n",
        "    line = [word.lower() for word in line]\n",
        "    # remove punctuation from each token\n",
        "    line = [word.translate(table) for word in line]\n",
        "    # remove non-printable chars form each token\n",
        "    line = [re_print.sub('', w) for w in line]\n",
        "    # remove tokens with numbers in them\n",
        "    line = [word for word in line if word.isalpha()]\n",
        "    # store as string\n",
        "    clean_pair.append(' '.join(line))\n",
        "  cleaned.append(clean_pair)\n",
        "\n",
        "pairs_cleaned = array(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2imz1YrL-Ft",
        "colab_type": "text"
      },
      "source": [
        "###Save cleaned text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvvDIqKiMDcO",
        "colab_type": "code",
        "outputId": "09d99db3-a384-469c-dddf-2bbfa4ae481f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def save_data(data, filename):\n",
        "  dump(data, open(filename, 'wb'))\n",
        "  print('Saved: %s' % filename)\n",
        "\n",
        "save_data(pairs_cleaned, 'eng-por.pkl')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: eng-por.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6S9SHZ4NX8E",
        "colab_type": "code",
        "outputId": "17b072db-76f6-4b9d-e3dc-3c3fb6e07d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(100):\n",
        "  print('[%s] => [%s]' % (pairs_cleaned[i,0], pairs_cleaned[i,1]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[go] => [vai]\n",
            "[go] => [va]\n",
            "[hi] => [oi]\n",
            "[run] => [corre]\n",
            "[run] => [corra]\n",
            "[run] => [corram]\n",
            "[run] => [corre]\n",
            "[run] => [corra]\n",
            "[run] => [corram]\n",
            "[who] => [quem]\n",
            "[who] => [que]\n",
            "[wow] => [uau]\n",
            "[wow] => [nossa]\n",
            "[wow] => [wow]\n",
            "[fire] => [fogo]\n",
            "[help] => [ajuda]\n",
            "[help] => [socorro]\n",
            "[jump] => [pule]\n",
            "[jump] => [pulem]\n",
            "[jump] => [pule]\n",
            "[stop] => [pare]\n",
            "[stop] => [parem]\n",
            "[wait] => [espere]\n",
            "[wait] => [espere]\n",
            "[wait] => [esperem]\n",
            "[go on] => [va]\n",
            "[hello] => [oi]\n",
            "[hello] => [alo]\n",
            "[hello] => [ola]\n",
            "[hello] => [alo]\n",
            "[i ran] => [eu corri]\n",
            "[i see] => [estou vendo]\n",
            "[i try] => [eu tento]\n",
            "[i try] => [tento]\n",
            "[i won] => [ganhei]\n",
            "[i won] => [eu venci]\n",
            "[oh no] => [ah nao]\n",
            "[relax] => [relaxe]\n",
            "[relax] => [relaxa]\n",
            "[shoot] => [tiro]\n",
            "[smile] => [sorria]\n",
            "[smile] => [sorriam]\n",
            "[attack] => [atacar]\n",
            "[attack] => [ataquem]\n",
            "[attack] => [ataque]\n",
            "[cheers] => [saude]\n",
            "[freeze] => [parado]\n",
            "[get up] => [levantese]\n",
            "[get up] => [levantemse]\n",
            "[get up] => [levantate]\n",
            "[get up] => [levantese]\n",
            "[get up] => [levantate]\n",
            "[go now] => [va agora]\n",
            "[got it] => [entendi]\n",
            "[got it] => [eu entendi]\n",
            "[got it] => [saquei]\n",
            "[got it] => [entendeu]\n",
            "[he ran] => [ele correu]\n",
            "[he ran] => [ele corria]\n",
            "[hop in] => [sobe ai]\n",
            "[hop in] => [entra ai]\n",
            "[hug me] => [me abrace]\n",
            "[i fell] => [eu cai]\n",
            "[i know] => [eu sei]\n",
            "[i know] => [sei]\n",
            "[i left] => [eu sai]\n",
            "[i paid] => [eu paguei]\n",
            "[i quit] => [eu me demito]\n",
            "[i work] => [eu estou trabalhando]\n",
            "[im ok] => [estou bem]\n",
            "[im ok] => [eu vou bem]\n",
            "[im up] => [estou acordado]\n",
            "[listen] => [escute]\n",
            "[listen] => [oucame]\n",
            "[listen] => [escuta]\n",
            "[listen] => [escutem]\n",
            "[listen] => [ouca isso]\n",
            "[listen] => [escutemme]\n",
            "[listen] => [escute]\n",
            "[listen] => [escuta]\n",
            "[listen] => [escutem]\n",
            "[listen] => [escutai]\n",
            "[no way] => [de jeito nenhum]\n",
            "[no way] => [impossivel]\n",
            "[no way] => [de maneira alguma]\n",
            "[no way] => [de modo algum]\n",
            "[no way] => [sem chance]\n",
            "[really] => [serio]\n",
            "[really] => [e mesmo]\n",
            "[really] => [mesmo]\n",
            "[really] => [e serio]\n",
            "[thanks] => [obrigado]\n",
            "[thanks] => [obrigada]\n",
            "[thanks] => [obrigado]\n",
            "[thanks] => [obrigada]\n",
            "[try it] => [tentao]\n",
            "[try it] => [proveo]\n",
            "[try it] => [provea]\n",
            "[we try] => [tentamos]\n",
            "[we try] => [nos tentamos]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxVZG3EqRnJY",
        "colab_type": "text"
      },
      "source": [
        "##Train/Test Separation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJItOhB-TbxS",
        "colab_type": "code",
        "outputId": "33636767-ad9e-408f-b3e2-6a2da0018dfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "n_sentences = 30000\n",
        "dataset = pairs_cleaned[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:27000], dataset[27000:]\n",
        "# save\n",
        "save_data(dataset, 'eng-por-both.pkl')\n",
        "save_data(train, 'eng-por-train.pkl')\n",
        "save_data(test, 'eng-por-test.pkl')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: eng-por-both.pkl\n",
            "Saved: eng-por-train.pkl\n",
            "Saved: eng-por-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9Bh4WQ_XR4r",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6823RGOMXU15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAeLmzEWatiy",
        "colab_type": "text"
      },
      "source": [
        "## Max Lenght"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgoQR7FlawwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvI65ElObIpY",
        "colab_type": "code",
        "outputId": "be0c62d4-322e-4807-df39-d7efd8d05645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare portuguese tokenizer\n",
        "por_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "por_vocab_size = len(por_tokenizer.word_index) + 1\n",
        "por_length = max_length(dataset[:, 1])\n",
        "print('Portuguese Vocabulary Size: %d' % por_vocab_size)\n",
        "print('Portuguese Max Length: %d' % (por_length))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 4257\n",
            "English Max Length: 6\n",
            "Portuguese Vocabulary Size: 7325\n",
            "Portuguese Max Length: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMQqWckAhuIc",
        "colab_type": "text"
      },
      "source": [
        "#Neural Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL0fpftarvQv",
        "colab_type": "text"
      },
      "source": [
        "##Encode with one hot encode (word embedding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUvp7q4HhxLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sequences(tokenizer, length, lines):\n",
        "  X = tokenizer.texts_to_sequences(lines)\n",
        "  X = pad_sequences(X, maxlen=length, padding='post')\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpq3mKOCtglj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_output(sequences, vocab_size):\n",
        "  ylist = list()\n",
        "  for sequence in sequences:\n",
        "    encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "    ylist.append(encoded)\n",
        "  y = array(ylist)\n",
        "  y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "  return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhUZu_EkxndZ",
        "colab_type": "text"
      },
      "source": [
        "##Prepare test and training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bY0hafyxrIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = encode_sequences(eng_tokenizer, eng_length, train[:,0])\n",
        "trainY = encode_sequences(por_tokenizer, por_length, train[:,1])\n",
        "trainY = encode_output(trainY, por_vocab_size)\n",
        "\n",
        "testX = encode_sequences(eng_tokenizer, eng_length, test[:,0])\n",
        "testY = encode_sequences(por_tokenizer, por_length, test[:,1])\n",
        "testY = encode_output(testY, por_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swt0xw2D8k1l",
        "colab_type": "text"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0uNPMwS8pRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "  model.add(LSTM(n_units))\n",
        "  model.add(RepeatVector(tar_timesteps))\n",
        "  model.add(LSTM(n_units, return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4PMQyVQ-0Qw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "0974f040-feda-45b4-c719-00d231dd3f1e"
      },
      "source": [
        "model = define_model(eng_vocab_size, por_vocab_size, eng_length, por_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "print(model.summary())\n",
        "#plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 6, 256)            1089792   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 12, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 12, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 12, 7325)          1882525   \n",
            "=================================================================\n",
            "Total params: 4,022,941\n",
            "Trainable params: 4,022,941\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTx1kpnXAs15",
        "colab_type": "text"
      },
      "source": [
        "##Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDidyzXLAvLc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2cc780cd-bd1a-450a-f833-49f219935b8b"
      },
      "source": [
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 27000 samples, validate on 3000 samples\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 140s - loss: 2.3626 - val_loss: 1.9685\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.96854, saving model to model.h5\n",
            "Epoch 2/30\n",
            " - 136s - loss: 1.9034 - val_loss: 1.8833\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.96854 to 1.88327, saving model to model.h5\n",
            "Epoch 3/30\n",
            " - 137s - loss: 1.8303 - val_loss: 1.8382\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.88327 to 1.83818, saving model to model.h5\n",
            "Epoch 4/30\n",
            " - 136s - loss: 1.7485 - val_loss: 1.7460\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.83818 to 1.74605, saving model to model.h5\n",
            "Epoch 5/30\n",
            " - 136s - loss: 1.6388 - val_loss: 1.6346\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.74605 to 1.63458, saving model to model.h5\n",
            "Epoch 6/30\n",
            " - 137s - loss: 1.4953 - val_loss: 1.5026\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.63458 to 1.50255, saving model to model.h5\n",
            "Epoch 7/30\n",
            " - 136s - loss: 1.3502 - val_loss: 1.3898\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.50255 to 1.38980, saving model to model.h5\n",
            "Epoch 8/30\n",
            " - 136s - loss: 1.2241 - val_loss: 1.3023\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.38980 to 1.30228, saving model to model.h5\n",
            "Epoch 9/30\n",
            " - 135s - loss: 1.1162 - val_loss: 1.2310\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.30228 to 1.23099, saving model to model.h5\n",
            "Epoch 10/30\n",
            " - 135s - loss: 1.0201 - val_loss: 1.1704\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.23099 to 1.17043, saving model to model.h5\n",
            "Epoch 11/30\n",
            " - 136s - loss: 0.9318 - val_loss: 1.1302\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.17043 to 1.13024, saving model to model.h5\n",
            "Epoch 12/30\n",
            " - 136s - loss: 0.8499 - val_loss: 1.0749\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.13024 to 1.07488, saving model to model.h5\n",
            "Epoch 13/30\n",
            " - 136s - loss: 0.7756 - val_loss: 1.0382\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.07488 to 1.03823, saving model to model.h5\n",
            "Epoch 14/30\n",
            " - 135s - loss: 0.7070 - val_loss: 1.0080\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.03823 to 1.00796, saving model to model.h5\n",
            "Epoch 15/30\n",
            " - 136s - loss: 0.6459 - val_loss: 0.9823\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.00796 to 0.98225, saving model to model.h5\n",
            "Epoch 16/30\n",
            " - 137s - loss: 0.5928 - val_loss: 0.9615\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.98225 to 0.96154, saving model to model.h5\n",
            "Epoch 17/30\n",
            " - 136s - loss: 0.5460 - val_loss: 0.9480\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.96154 to 0.94801, saving model to model.h5\n",
            "Epoch 18/30\n",
            " - 137s - loss: 0.5049 - val_loss: 0.9400\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.94801 to 0.93998, saving model to model.h5\n",
            "Epoch 19/30\n",
            " - 135s - loss: 0.4698 - val_loss: 0.9332\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.93998 to 0.93323, saving model to model.h5\n",
            "Epoch 20/30\n",
            " - 136s - loss: 0.4385 - val_loss: 0.9249\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.93323 to 0.92490, saving model to model.h5\n",
            "Epoch 21/30\n",
            " - 135s - loss: 0.4128 - val_loss: 0.9248\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.92490 to 0.92485, saving model to model.h5\n",
            "Epoch 22/30\n",
            " - 137s - loss: 0.3893 - val_loss: 0.9242\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.92485 to 0.92416, saving model to model.h5\n",
            "Epoch 23/30\n",
            " - 137s - loss: 0.3684 - val_loss: 0.9244\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.92416\n",
            "Epoch 24/30\n",
            " - 137s - loss: 0.3501 - val_loss: 0.9239\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.92416 to 0.92390, saving model to model.h5\n",
            "Epoch 25/30\n",
            " - 136s - loss: 0.3335 - val_loss: 0.9282\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.92390\n",
            "Epoch 26/30\n",
            " - 136s - loss: 0.3189 - val_loss: 0.9419\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.92390\n",
            "Epoch 27/30\n",
            " - 142s - loss: 0.3050 - val_loss: 0.9403\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.92390\n",
            "Epoch 28/30\n",
            " - 138s - loss: 0.2929 - val_loss: 0.9452\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.92390\n",
            "Epoch 29/30\n",
            " - 165s - loss: 0.2818 - val_loss: 0.9529\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.92390\n",
            "Epoch 30/30\n",
            " - 148s - loss: 0.2714 - val_loss: 0.9562\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.92390\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6c5df33a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ptSIII4xGp9",
        "colab_type": "text"
      },
      "source": [
        "#Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JlO5PD3bfPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == integer:\n",
        "      return word\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shg1iCrqb_5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_sequence(model, tokenizer, source):\n",
        "  prediction = model.predict(source, verbose=0)[0]\n",
        "  integers = [argmax(vector) for vector in prediction]\n",
        "  target = list()\n",
        "  for i in integers:\n",
        "    word = word_for_id(i, tokenizer)\n",
        "    if word is None:\n",
        "      break\n",
        "    target.append(word)\n",
        "  return ' '.join(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lycid3ryxJok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, tokenizer, source)\n",
        "\t\traw_target, raw_src = raw_dataset[i,1], raw_dataset[i,0]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhBz6vUrdLiw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "e0644c53-e843-4985-eef1-412db1842b82"
      },
      "source": [
        "model = load_model('model.h5')\n",
        "print('Train')\n",
        "evaluate_model(model, por_tokenizer, trainX, train)\n",
        "print('Test')\n",
        "evaluate_model(model, por_tokenizer, testX, test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "src=[listen to the rain], target=[escute a chuva], predicted=[escutem a chuva]\n",
            "src=[hang on], target=[aguardem], predicted=[aguardem]\n",
            "src=[many people hunt], target=[muitas pessoas cacam], predicted=[muitas pessoas cacam]\n",
            "src=[let me help you], target=[deixeme te ajudar], predicted=[deixeme ajudala]\n",
            "src=[he works very hard], target=[ele trabalha bastante], predicted=[ele trabalha muito]\n",
            "src=[ive been running], target=[tenho corrido], predicted=[eu estive correndo]\n",
            "src=[hows tom doing], target=[como o tom esta], predicted=[como o tom esta indo]\n",
            "src=[he lives in boston], target=[ele mora em boston], predicted=[ele vive em boston]\n",
            "src=[we have a plan], target=[temos um plano], predicted=[nos temos plano plano]\n",
            "src=[tom has arrived], target=[tom chegou], predicted=[tom chegou]\n",
            "BLEU-1: 0.766687\n",
            "BLEU-2: 0.672548\n",
            "BLEU-3: 0.606311\n",
            "BLEU-4: 0.438873\n",
            "Test\n",
            "src=[youre taunting me], target=[estas a tratarme com sarcasmo], predicted=[a senhora de de]\n",
            "src=[ill never tell], target=[eu nunca vou contar], predicted=[eu nunca vou dizer]\n",
            "src=[is tom still there], target=[tom ainda esta la], predicted=[o tom esta esta]\n",
            "src=[this is astounding], target=[isto e impressionante], predicted=[isto e refrescante]\n",
            "src=[how is your cold], target=[como esta o seu resfriado], predicted=[como esta a resfriado]\n",
            "src=[i wont bite], target=[nao irei morder], predicted=[eu nao vou morder]\n",
            "src=[we wonder why], target=[nos nos perguntamos o porque], predicted=[nos nos o porque]\n",
            "src=[this is the boy], target=[eis o menino], predicted=[este e o garoto]\n",
            "src=[is it clean], target=[esta limpo], predicted=[e e]\n",
            "src=[tom is bad], target=[tom e mau], predicted=[tom tom esta]\n",
            "BLEU-1: 0.581548\n",
            "BLEU-2: 0.449631\n",
            "BLEU-3: 0.366934\n",
            "BLEU-4: 0.218413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taNVqg-O-axM",
        "colab_type": "text"
      },
      "source": [
        "#Translating input text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNesep5iChSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_text(model, src_text, src_tokenizer, src_length, tar_tokenizer):\n",
        "  ltext = list()\n",
        "  ltext.append(src_text)\n",
        "  enconded_text = encode_sequences(src_tokenizer, src_length, ltext)\n",
        "  translated_text = predict_sequence(model, tar_tokenizer, enconded_text)\n",
        "  print('[%s] => [%s]' % (src_text, translated_text))\n",
        "  return translated_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h03x-IFA-hW0",
        "colab_type": "code",
        "outputId": "77915ecd-965c-4bd4-c34e-79292de9176d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "model = load_model('model.h5')\n",
        "eng_text = input('Write a phrase to translate: ')\n",
        "translation = translate_text(model, eng_text, eng_tokenizer, eng_length, por_tokenizer)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Write a phrase to translate: i have an idea\n",
            "[i have an idea] => [tenho uma uma ideia]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}